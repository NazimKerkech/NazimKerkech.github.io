<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://nazimkerkech.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nazimkerkech.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-07T14:03:00+00:00</updated><id>https://nazimkerkech.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">SAT Solvers</title><link href="https://nazimkerkech.github.io/blog/2020/SAT-SOLVERS/" rel="alternate" type="text/html" title="SAT Solvers"/><published>2020-06-30T00:00:00+00:00</published><updated>2020-06-30T00:00:00+00:00</updated><id>https://nazimkerkech.github.io/blog/2020/SAT-SOLVERS</id><content type="html" xml:base="https://nazimkerkech.github.io/blog/2020/SAT-SOLVERS/"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js" integrity="sha512-EKW5YvKU3hpyyOcN6jQnAxO/L8gts+YdYV6Yymtl8pk9YlYFtqJgihORuRoBXK8/cOIlappdU6Ms8KdK6yBCgA==" crossorigin="anonymous" referrerpolicy="no-referrer"> </script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css"/> <script src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js"> </script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css"/> <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"> </script> <h1 id="introduction">Introduction</h1> <p>We discussed in a previous post about SAT and its theoretical context. SAT is a decidable problem in logic and theoretical computer science and it is $\mathsf{NP}$-complete. This means that for every satisfiable instance of the problem (i.e. for which the answer is “yes”), one of the branches of the nondeterministic algorithm solving SAT would lead to a satisfaction of the instance in a polynomial time. Some manipulations like resolution, subsumption, or merging could simplify the instance and sometimes help us conclude the inconsistency of a branch before reaching the leaves. It is then evident that the key to efficiently solve SAT is to know at least one of these branches that leads to satisfaction beforehand. This boils down to “judiciously” traversing the execution tree of the nondeterministic algorithm and thus choosing the propositions to assign whenever the choice presents itself (for every node of the tree).</p> <p>SAT’s difficulty is only practical and not theoretical.Theory just doesn’t allow us to know a priori this polynomial complexity branch or even whether it exists (for inconsistent instances). It is because of this theoretical dead-end that we implement in practice an empirical approach to the problem to effectively solve SAT through heuristics.</p> <p>SAT solvers fall into two categories: incomplete approaches, such as local search or evolutionary genetic algorithms, which do not guarantee finding a solution and consist of, for local search, judiciously navigating the valuation space until finding one that satisfies the formula within a predefined time. Failing that, it cannot conclude. And, for genetic algorithms, generating random valuations and selecting those that appear closest to a formula-satisfying valuation, then applying minimal changes and continuing until finding one; otherwise, if the allotted time is exceeded, it will be impossible to conclude. And the complete approaches. These are based on the principles of enumeration and resolution. We will discuss some of these algorithms in this chapter.</p> <hr/> <h1 id="enumeration">Enumeration</h1> <h2 id="semantic-tree">Semantic Tree</h2> <p>The Semantic Tree is a naive method that constructs a binary tree where each edge represents a valuation of a variable and exhaustively traverses each branch using depth-first search and evaluates the formula once it reaches a leaf until finding one that satisfies the instance.</p> <h2 id="quine-algorithm">Quine Algorithm</h2> <p>The Quine algorithm is a refinement of the semantic trees algorithm. The difference is that it allows to conclude the inconsistency of a branch (and thus of all its leaves) upstream by performing partial evaluations at each node.</p> <pre id="quine" class="pseudocode">
    % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
    \begin{algorithm}
    \caption{Quine}
    \begin{algorithmic}
      \INPUT $\mathcal{F}$
      \IF{ $\mathcal{F} = \emptyset \ $ } 
        \RETURN SAT
      \ELIF{$\perp \in \mathcal{F}$}
        \Return INSAT
      \ENDIF
      \STATE $l \ \gets \ BranchingHeuristics (\mathcal{F})$
      \RETURN (QUINE($\mathcal{F}_{|l} $) OR QUINE($\mathcal{F}_{|\neg l} $))
    \end{algorithmic}
    \end{algorithm}
</pre> <script>pseudocode.renderElement(document.getElementById("quine"));</script> <hr/> <h1 id="resolution">Resolution</h1> <h2 id="resolution-principle">Resolution Principle</h2> <p>The resolution rule or Robinson’s resolution principle consists of iteratively applying resolution derivations to the clauses of a formula. A resolution derivation is a sequence $\Pi [ c_{1},\ …,\ c_{n} = c]$ such that for every $i \in { 1,\ …,\ k } $ either $c_{i} \in \mathcal{F}$ or $c_{k}=\eta [x, c_{i}, c_{j}] $ with $i, j \in { 1,\ …,\ k } $.</p> <p>Resolution, subsumption, and merging rules establishes a complete automatic demonstration method. Resolution alone is complete for refutation (i.e. inconsistant formulas)<d-cite key="Konlac2014"></d-cite>. The problem in implementing this kind of procedure is that there is no method to determine which resolution to perform and when. We might end up generating an exponential number of clauses. To address this, a number of orientations and restrictions for building a resolution derivation are proposed, among them the Davis-Putnam algorithm.</p> <h2 id="davis-putnam-algorithm">Davis-Putnam Algorithm</h2> <p>The Davis-Putnam algorithm <d-cite key="DBLP:journals/jacm/DavisP60"></d-cite> consists of choosing at each iteration a propositional variable to eliminate from the formula and exhaustively applying resolution between the subsets $\mathcal{F}_{|x}$ and $ \mathcal{F} _{ | \neg x} $ of clauses of $ \mathcal{F} $ containing respectively the propositional variable $x$ and its negation. Then, removing them from the formula and replacing them with the set of clauses $\eta [x,\ \mathcal{F} _{ | x},\ \mathcal{F} _{ | \neg x} ] $ obtained by applying resolution between each clause of $ \mathcal{F} _{ | x} $ with each clause of $ \mathcal{F} _{ | \neg x} $, and simplifying the obtained clause.</p> <p>The complexity of the algorithm is $O(2^{n})$ where $n$ is the number of variables in the formula.</p> <pre id="DavisPutnam" class="pseudocode">
    \begin{algorithm}
    \caption{Davis-Putnam}
    \begin{algorithmic}
      \INPUT $\mathcal{F}$
      \WHILE{ $V_\mathcal{F} = \emptyset \ $ } 
        \STATE $V_\mathcal{F}$ being the set of all propositionnal variables of $\mathcal{F}$
        \STATE $x \ \gets \ BranchingHeuristics (\mathcal{F})$
        \STATE $\mathcal{V}_{\mathcal{F}}\ \gets \ \mathcal{V}_{\mathcal{F}} \setminus \{x\}$ 
	      \STATE $\mathcal{F} \ \gets \ \mathcal{F} \ \setminus \ (\mathcal{F}_{x} \ \cup \ \mathcal{F}_{\neg x})$ 
	      \STATE $\mathcal{F} \ \gets \ \mathcal{F} \ \cup \ \eta [x,\ \mathcal{F}_{x}, \  \mathcal{F}_{\neg x}]$       
        \IF{$\perp \in \mathcal{F}$}
          \Return INSAT
        \ENDIF
      \ENDWHILE
      \RETURN SAT
    \end{algorithmic}
    \end{algorithm}
</pre> <script>pseudocode.renderElement(document.getElementById("DavisPutnam"));</script> <hr/> <h1 id="efficient-solvers">Efficient Solvers</h1> <h2 id="dpll-algorithm">DPLL Algorithm</h2> <p>To understand the DPLL algorithm <d-cite key="DBLP:journals/cacm/DavisLL62"></d-cite>, we will introduce a set of concepts :</p> <h4 id="unit-propagation">Unit Propagation</h4> <ul> <li>If a formula $\mathcal{F}$ contains a unit atom $x$, i.e., an atom that appears in a clause containing only itself, then the formula $\mathcal{F}$ is equivalent to $\mathcal{F}_{|x}$ obtained by assigning $x$ to true, or in other words, by removing the clauses from $\mathcal{F}$ containing $x$ and removing $\neg x$ from clauses containing it as follows: \(\mathcal{F}_{|x} = \{c | c \in \mathcal{F}, \{x, \neg x \} \cap c = \emptyset \} \cup \{c \backslash \{\neg x \} | c \in \mathcal{F}, \neg x \in c \}\)</li> <li>$\mathcal{F}^{*}$ is recursively defined as the formula obtained by applying unit propagation on $\mathcal{F}$ as follows: <ul> <li>$ \mathcal{F}^{*}= \mathcal{F} $ if $ \mathcal{F} $ doesn’t contains any unit clause</li> <li>$ \mathcal{F}^{*}= \perp $ if $ \mathcal{F} $ contains both unit clauses $x$ and $ \neg x $</li> <li>$ \mathcal{F}^{*}= (\mathcal{F}_{|x})^{*}$ if $\mathcal{F}$ contains a unit clause containing the literal $x$.</li> </ul> </li> </ul> <h4 id="propagation-sequence">Propagation Sequence</h4> <ul> <li>A propagation sequence $\mathcal{P} = \langle x_{1},\ …,\ x_{n} \rangle $ is a sequence of literals for a CNF $\mathcal{F}$ such that $\forall i \in [1,\ n] $, the unit clause $ x _{i} \in \mathcal{F} _{ | x _{1},\ …,\ x _{i-1}} $</li> </ul> <h4 id="decision-propagation-sequence">Decision-Propagation Sequence</h4> <ul> <li>A decision-propagation sequence $\mathcal{P} = \langle (x),\ x_{1},\ …,\ x_{n} \rangle $ is a propagation sequence for a CNF $\mathcal{F} \land (x)$.</li> </ul> <h4 id="propagation-stack">Propagation Stack</h4> <ul> <li>Considering a sequence of atoms $ [ x _{0} ,\ …,\ x _{n} ] $ which we will call a decision sequence, a propagation stack $ \mathcal{H}= { \mathcal{S} _{0} ,\ …,\ \mathcal{S} _{n} } $ is a sequence of decision-propagation sequences $ \mathcal{S} _{i} $ such that $ \forall \ i \ \mathcal{S} _{i} \ is \ a \ DPS \ considering \ ( \mathcal{F} _{ | x _{0} , \ …, \ x _{i-1} }) ^{ * } $</li> </ul> <p>The DPLL algorithm consists of iteratively applying decision-propagation sequences on the instance we want to solve.</p> <pre id="DPLL" class="pseudocode">
    \begin{algorithm}
    \caption{Davis-Putnam}
    \begin{algorithmic}
      \INPUT $\mathcal{F}$
      \STATE $\mathcal{F} \ \gets \ PropagationUnitaire(\mathcal{F})$ \;
      \IF{ $\mathcal{F} = \emptyset \ $ }
        \Return SAT
      \IF{$\perp \in \mathcal{F}$}
          \Return INSAT
      \STATE $l \ \gets \ BranchingHeuristics (\mathcal{F})$
      \Return (DPLL($\mathcal{F} \land \{l\} $) OR DPLL($\mathcal{F} \land \{\neg l\} $))
    \end{algorithmic}
    \end{algorithm}
</pre> <script>pseudocode.renderElement(document.getElementById("DPLL"));</script> <p>\Retour (DPLL($\mathcal{F} \land {l} $) ou DPLL($\mathcal{F} \land {\neg l} $)) \caption{Algorithme DPLL} \end{algorithm}</p> <hr/> <h2 id="cdcl-type-algorithms">CDCL Type Algorithms</h2> <p>CDCL (Conflict Driven Clause Learning) type algorithms add on the DPLL algorithm clause learning techniques that are performed at each conflict leading to non-chronological backtracks. CDCL solvers can also implement, in addition to the basic functions of DPLL and clause learning, various components that optimize the algorithm: preprocessing, lazy data structures, heuristics, and restarts.</p> <p>In this section, we will primarily focus on the components of the Chaff algorithm <d-cite key="DBLP:conf/dac/MoskewiczMZZM01"></d-cite>. The general architecture of CDCL solvers is as follows:</p> <pre id="CDCL" class="pseudocode">
    \begin{algorithm}
    \caption{CDCL}
    \begin{algorithmic}
    \INPUT $\mathcal{F}$

      \STATE $\Delta \ \gets \ \emptyset$ (Database of learned clauses)
      \STATE $I_{p} \ \gets \ \emptyset$ (Partial interpretation)
      \STATE $dl \ \gets \ 0$ (Decision level)

      \WHILE{True}
          \STATE $\alpha \ \gets \ \text{Unit Propagation}(\mathcal{F} \cup \Delta, \ I_{p} )$ ;
          \IF{$(\alpha = null)$}
              \STATE $x \ \gets \ \text{Branching Heuristic}(\mathcal{F} \cup \Delta, I_{p} )$ ;
              \IF{all variables are assigned}
                  \STATE return SAT 
              \ENDIF
              \STATE $l \ \gets \ \text{Choose Polarity}(x)$ 
              \STATE $dl \gets dl + 1 $ 
              \STATE $I_{p} \gets I_{p} \cup \{l^{dl} \}$ 
              \IF{Perform Reduction()}
                  \STATE $ReductionClausesApprises(\Delta)$ 
              \ENDIF
          \ELSE
              \IF{$dl = 0$}
                  \Return UNSAT
              \ENDIF
              \STATE $\beta, \ bl \ \gets \ \text{Conflict Analysis}(\mathcal{F} \cup \Delta, I_{p} , \alpha)$ ;
              \STATE $\Delta \ \gets \ \Delta \ \cup \beta$ ;
              \STATE $Backtrack(\mathcal{F} \cup \Delta, \ I_{p} , \ bl)$ ;
              \IF{Perform Restart()}
                  \STATE $Restart(I_{p},\ dl)$ ;
              \ENDIF
          \ENDIF
      \ENDWHILE
    \end{algorithmic}
    \end{algorithm}
</pre> <script>pseudocode.renderElement(document.getElementById("CDCL"));</script> <h4 id="lazy-data-structures">Lazy Data Structures</h4> <p>During unit propagation, the algorithm searches for the possible set of literals involved and conflicts generated by the valuation of the literal that is currently propagated. To achieve this, the naive method would be to exhaustively traverse all the clauses of the formula. However, Zhao, Zhang, and Malik propose in <d-cite key="DBLP:conf/dac/MoskewiczMZZM01"></d-cite> an optimization where a clause needs to be visited only when the number of falsified literals in this clause of size $N$ changes from $N-2$ to $N-1$ (making it a unit clause and thus more critical). They suggest choosing two unassigned literals for each clause. This ensures that until either of these two literals is falsified, the clause is neither unit nor falsified.</p> <h4 id="non-chronological-backtrack">Non-Chronological Backtrack</h4> <p>In cases where a conflict arises due to an assignment made several levels upstream, multiple backtracks may be needed to resolve the conflict. It is possible to precisely identify the last assignment causing the conflict by learning a new clause, which, once the non-chronological backtrack is performed, prevents making the same mistake again by propagating the variable that caused the conflict. To understand this, we define a set of concepts:</p> <h5 id="conflict-reason">Conflict Reason:</h5> <p>For a formula $\mathcal{F}$ in CNF and a partial interpretation $I_{p}$ obtained by propagation from $\mathcal{F}$, the conflict reasons of a literal $l \in I_{p}$ is the set of clauses (denoted $\overrightarrow{reason}(l)$) $C$ of $\mathcal{F}$ such that $l \in C$ and for any other $y$ in $C$: $\neg y \in I_{p}$ and $y$ precedes $l$ in the interpretation $I_{p}$.</p> <p>If there exists $y \in C$ such that $\neg y \notin I_{p}$ (this means that $l$ is not a result of unit propagation), then $\overrightarrow{reason}(l) = \perp$.</p> <p>We only need to consider a single reason clause for a propagated literal.</p> <h5 id="explanation">Explanation:</h5> <p>For a formula $\mathcal{F}$ in CNF and a partial interpretation $I_{p}$ obtained by propagation from $\mathcal{F}$ using the decision sequence $\delta = [x_{1} ,\ x_{2} ,\ . . . ,\ x_{n}]$, and for a literal $l \in I_{p}$, the explanation of $l$ (denoted $exp(l)$) is $\emptyset$ if $l \in \delta$, or the set of $y \in I_{p}$ such that $\neg y \in I_{p}$ otherwise.</p> <h5 id="implication-graph">Implication Graph:</h5> <p>For a formula $\mathcal{F}$ in CNF and a partial interpretation $I_{p}$ obtained by propagation from $\mathcal{F}$, the implication graph is a directed graph $ G = (N, A) $ where the set of nodes $N$ is the set of literals of $I_{p}$, and $(a,\ b)\ \in \ A $ if and only if $b\ \in \ exp(a)$.</p> <h5 id="node-dominating-a-level">Node Dominating a Level:</h5> <p>Given $\mathcal{F}$ a formula, $I_{p}$ a partial interpretation, and $G = (N,\ A)$ the implication graph generated from $\mathcal{F}$ and $I_{p}$, a node $x \in N$ dominates a node $y \in N$ if and only if $lvl(x) = lvl(y)$ and $\forall z \in N$, with $lvl(z) = lvl(x)$, all paths covering $z$ and $y$ pass through $x$.</p> <h5 id="unique-implication-point-uip">Unique Implication Point (UIP):</h5> <p>Given $\mathcal{F}$ a formula, $I_{p}$ a conflicting partial interpretation, and $ G = (N,\ A)$ the implication graph generated from $\mathcal{F}$ based on $I_{p}$. The node $x$ is a unique implication point if and only if $x$ dominates the conflict.</p> <p>The authors of <d-cite key="DBLP:conf/dac/MoskewiczMZZM01"></d-cite> propose considering, during conflict analysis, only the closest UIP to the conflict, called F-UIP. It is the propagation of this unique implication point that led to the conflict. It is itself implied by the propagation of another literal at a previous level. The goal is to highlight this implicit implication by learning a clause from this conflict, and then backtrack to the level where the learned clause became unitary, i.e., the last level where another literal of the clause (other than the F-UIP we found) was propagated.</p> <h4 id="proof-by-conflict-driven-resolution">Proof by Conflict-Driven Resolution</h4> <p>Given a formula $\mathcal{F}$ and a conflicting partial interpretation $I_{p}$ obtained by unit propagation, a proof by conflict-driven resolution $R$ is a sequence of clauses $\langle \delta_{1} ,\ \delta_{2} ,\ … ,\ \delta_{k} \rangle $ that satisfies the following conditions:</p> <ul> <li>$\delta_{1} = \eta [z, \overrightarrow{reason}(z), \overrightarrow{reason}(\neg z)]$, where ${z,\ \neg z }$ is a conflict;</li> <li>For every $i \in {2,\ …,\ k}$, $\delta_{i}$ is constructed by selecting a literal $y \in \delta_{i-1}$ for which $\overrightarrow{reason}(\neg y)$ is defined. Thus, $y \in \delta_{i-1}$ and $\neg y \in \overrightarrow{reason}(\neg y)$, allowing two clauses to enter resolution. The clause $\delta_{i}$ is defined as: \(\delta_{i} = \eta [y, \delta_{i-1}, \overrightarrow{reason}(\neg y)]\)</li> <li>$\delta_{k}$ is an assertive clause.</li> </ul> <p>It’s worth noting that each $\delta_{i}$ is a resolvent of formula $\mathcal{F}$: by induction, $\delta_{1}$ is the resolvent between two clauses of $\mathcal{F}$; for every $i &gt; 1$, $\delta_{i}$ is a resolvent between $\delta_{i-1}$ (which, by the induction hypothesis, is a resolvent) and a clause of $\mathcal{F}$. Each $\delta_{i}$ is also implied by $\mathcal{F}$, i.e., $F \models \delta_{i}$.</p>]]></content><author><name>Nazim Kerkech</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Propositional Satisfiability problem : Theoretical context</title><link href="https://nazimkerkech.github.io/blog/2020/SAT/" rel="alternate" type="text/html" title="The Propositional Satisfiability problem : Theoretical context"/><published>2020-06-30T00:00:00+00:00</published><updated>2020-06-30T00:00:00+00:00</updated><id>https://nazimkerkech.github.io/blog/2020/SAT</id><content type="html" xml:base="https://nazimkerkech.github.io/blog/2020/SAT/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Theoretical computer science is, with mathematics, a subject that holds the key to addressing some of the problems that transcend our existence<d-footnote>This phrasing is to be interpreted either in the weak sense, as mathematics being a tool that perfectly describes the universe and every phenomenon from the scale of the atom to human populations, or in the strongest sense of "transcendence" if we consider <a href="https://physics.mit.edu/faculty/max-tegmark/">Max Tegmark</a>'s proposition of the "<a href="https://arxiv.org/abs/0704.0646">Mathematical Universe</a>" </d-footnote>. $\mathsf{P}$ VS $\mathsf{NP}$ is one of such problems and SAT could be a starting point to answer this problem.</p> <p>Given a formula in Propositional Calculus (or a set of clauses), SAT consists of determining whether there exists a valuation of the atoms composing it such as that formula is True. This problem is fundamental due to its simplicity and context. Propositional calculus provides us with a set of concepts allowing us to fully grasp SAT.</p> <p>SAT is $\mathsf{NP}$. This means that a non-deterministic Turing machine can solve it in polynomial time. This implies that a deterministic Turing machine can solve it in exponential time (theorem); and thus that a modern computer can solve it (deterministically -by setting heuristics even arbitrary-) in exponential time. Therein lies the problem: $\mathsf{NP}$ problems present an asymmetry between instances for which the answer is “yes” and those for which the answer is “no”. For a set of satisfiable clauses (a satisfiable formula), the algorithm will stop at the first branch that would have lead to the satisfaction of the set (and in the best possible case -that is, if the algorithm has never “made a mistake” and has only traversed this path- its complexity will be polynomial<d-footnote>This is purely illustrative ; the 'probability' for a naive algorithm to do this tends exponentially to zero, and even if the heuristic eliminates an exponential number of branches, the algorithme would still have to traverse an exponential number of branches</d-footnote>). But for a set of inconsistent clauses (a non satisfiable formula), the algorithm will have to traverse all paths and thus exhaustively check all possible valuations of the formula’s variables ($2^n$) regardless of the heuristics it uses.</p> <p>Moreover SAT is $\mathsf{NP}$-Hard which means that every $\mathsf{NP}$ problem can be reduced to it. Therefor, solving SAT efficiently would be equivalent to doing so for every $\mathsf{NP}$ problem (given that the reduction algorithhm is efficient).</p> <p>We will begin by an introduction to Propositional calculus and formal systems setting the stage for a more formal definition of SAT.</p> <hr/> <h1 id="theoretical-context">Theoretical Context</h1> <h2 id="propositional-calculus">Propositional Calculus</h2> <p>Propositional calculus <d-cite key="alliot2002intelligence"></d-cite> is a formal system describing logic, the study of which consists of two distinct theories: a model theory and a proof theory, respectively studying the semantic and syntactic aspects of the language. Each has its own concepts and methods, which, although, as we will see later, equivalent, should not be confused.</p> <p>Understanding propositional calculus amounts to understanding the difficulty and giving ourselves the theoretical means for solving SAT.</p> <p>This section will be divided into four parts: In the first, we will see the specificities of the language; in the second and third respectively, the model theory and the proof theory; and finally, in the fourth, we will see the fundamental properties of the system.</p> <h3 id="language">Language</h3> <p>The formal language of propositional calculus is a formalization of logic. It considers propositions (or propositional variables) as well as the relationships that exist between them (conjunction, disjunction, implication, equivalence) and the negation of these. The set of propositional variables is denoted as $V_p$. We are going to define each element of the language.</p> <h4 id="proposition">Proposition</h4> <ul> <li>A proposition (or a propositional variable) is a closed statement, meaning it does not depend on any external parameter.</li> </ul> <h4 id="atom">Atom</h4> <ul> <li>An atom can be a propositional variable, a tautology $\top$, or a contradiction $\bot$. <d-cite key="alliot2002intelligence"></d-cite></li> </ul> <h4 id="formula">Formula</h4> <ul> <li>A formula is defined as follows<d-cite key="alliot2002intelligence"></d-cite>: <ul> <li>An atom is a formula;</li> <li>If A and B are formulas, then A$\rightarrow$B, A$\leftrightarrow$B, A$\land$B, A$\lor$B are formulas;</li> <li>If A is a formula, then $\neg$A is a formula.</li> </ul> </li> </ul> <h3 id="model-theory">Model Theory</h3> <p>Model theory considers the truth value, i.e., the truth or falsehood of a formula in propositional calculus. It assumes the principle of excluded middle ($\forall A \;\; \; \models A \lor \neg A$) and the postulate that the truth value of a formula is entirely determined by the truth value of each of its atoms. We will define the concepts that make up model theory.</p> <h4 id="valuation">Valuation</h4> <ul> <li>A valuation of a set of propositional variables $V \in V_p$ is a function $v : V \to {t, f}$ satisfying the conditions: <ul> <li>$v(\neg X) = \neg v(X)$</li> <li>$v(X \land Y) = v(X) \land v(Y)$</li> <li>$v(X \lor Y) =v(X) \lor v(Y)$</li> <li>$v(X \to Y) = v(X) \to v(Y)$</li> </ul> </li> </ul> <h4 id="interpretation">Interpretation</h4> <ul> <li>In propositional calculus, an interpretation<d-footnote>Not to be confused with the notion of interpretation for predicate calculus</d-footnote> of a formula $F$ is a valuation of all propositional variables appearing in $F$.</li> </ul> <h4 id="satisfaction">Satisfaction</h4> <ul> <li>An interpretation $I$ satisfies a formula $F$ if and only if it leads to a “true” value for $F$ in the truth table (by following the results of the definition table of each connector). We write $I \models F$.</li> </ul> <h4 id="valid-formula">Valid Formula</h4> <ul> <li>$F$ is a tautology or valid formula if and only if $F$ is satisfied by all possible interpretations. We write $\models F$.</li> <li>If a formula $B$ is true for all interpretations satisfying $A$, then $B$ is a valid consequence of $A$ and we write $A \models B$. If $B \models A$, then $\models (A \to B)$.</li> </ul> <h3 id="proof-theory">Proof Theory</h3> <p>Proof theory considers the provability of a formula: It admits a number of axioms as well as inference rules. A formula is provable (or is a theorem) if it is possible to derive it from the theorems and inference rules.</p> <h4 id="theorem">Theorem</h4> <ul> <li>A formula $A$ is a theorem, denoted $\vdash A$, if $A$ is an axiom or if $A$ is obtained from the application of an inference rule on other theorems. The axioms admitted by propositional calculus are: <ul> <li>$A\rightarrow(B\rightarrow A)$</li> <li>$(A\rightarrow(B\rightarrow C))\rightarrow ((A\rightarrow B)\rightarrow(A\rightarrow C))$</li> <li>$(\neg B\rightarrow \neg A)\rightarrow (A\rightarrow B)$</li> </ul> </li> <li>The axioms are tautologies.</li> </ul> <p>Inference rules are:</p> <ul> <li><strong>Modus Ponens</strong></li> </ul> <div style="margin-left: 50%">p</div> <div style="margin-left: 50%">p → q</div> <div style="margin-left: 50%">_________</div> <div style="margin-left: 50%">∴ q</div> <ul> <li><strong>Modus Tollens</strong></li> </ul> <div style="margin-left: 50%">¬q</div> <div style="margin-left: 50%">p → q</div> <div style="margin-left: 50%">_________</div> <div style="margin-left: 50%">∴ ¬p</div> <ul> <li><strong>Disjunctive Syllogism</strong></li> </ul> <div style="margin-left: 50%">¬q</div> <div style="margin-left: 50%">p ∨ q</div> <div style="margin-left: 50%">_________</div> <div style="margin-left: 50%">∴ p</div> <ul> <li><strong>Hypothetical Syllogism</strong></li> </ul> <div style="margin-left: 50%">p → q</div> <div style="margin-left: 50%">q → r</div> <div style="margin-left: 50%">_________</div> <div style="margin-left: 50%">∴ p → r</div> <h4 id="proof">Proof</h4> <ul> <li> <p>A proof of a theorem $A$ is a finite sequence ($A_{1}$, $A_{2}$, …, $A_{i}$, …, $A$) where each $A_{i}$ is either an axiom or obtained by applying an inference rule to previous theorems $A_{j}$ and $A_{k}$. We write $\vdash A$.</p> </li> <li> <p>If we admit that there may be $A_{i}$ that are not theorems, we say that $A$ is deduced from the set of $A_{i}$ that are not theorems, and it is denoted $A_{m}$, …, $A_{n} \vdash A$.</p> </li> </ul> <h4 id="deduction-theorem">Deduction Theorem</h4> <ul> <li>If $A \vdash B$, then $\vdash (A \to B)$</li> </ul> <h4 id="inconsistent-formula">Inconsistent Formula</h4> <ul> <li>A formula is inconsistent if and only if it is possible to deduce a contradiction $\bot$. A formula is consistent otherwise.</li> </ul> <h3 id="fundamental-properties">Fundamental Properties</h3> <p><strong>Definition: Adequacy</strong></p> <ul> <li>A logic is said to be adequate if every theorem is a tautology. Propositional calculus is adequate.</li> </ul> <p><strong>Definition: Completeness</strong></p> <ul> <li>A logic is said to be complete if every tautology is a theorem. Propositional calculus is complete. Furthermore, if for every formula $F$ being a valid consequence of another formula $A$, then $F$ can be deduced from $A$. In this case, propositional calculus is said to be strongly complete.</li> </ul> <p><strong>Definition: Consistency</strong></p> <ul> <li>A logic is said to be consistent if there is no formula $A$ such that both $\vdash A$ and $\vdash \neg A$. Propositional calculus is consistent.</li> </ul> <p><strong>Definition: Syntactic Completeness</strong></p> <ul> <li>A logic is said to be syntactically complete if and only if, for every formula $A$, either $\vdash A$ or $\vdash \neg A$. Propositional calculus is syntactically complete.</li> </ul> <p><strong>Definition: Decidability</strong></p> <ul> <li>A logic is said to be decidable when a mechanical procedure exists that can establish, in a finite time, whether a given formula is a theorem or not. Propositional calculus is decidable.</li> </ul> <h2 id="results-in-logic">Results in Logic</h2> <p>A set $E$ of formulas is unsatisfiable or semantically inconsistent if and only if there exists no interpretation $M$ such that every formula $A$ in $E$ is satisfied by $M$. An antilogy logically implies any formula and in particular $\emptyset$.</p> <p><strong>Deduction Principle Theorem</strong></p> <p>A formula $A$ is a valid consequence of a set $E$ of formulas if and only if $E \cup {\neg A}$ is unsatisfiable.</p> <p>The deduction principle reduces prooving a deduction to prooving the inconsistency of a set of formulas.</p> <p><strong>Resolution</strong></p> <p>Given two clauses $c_{i} = (x \lor \alpha_{i} )$ and $c_{j} = ((\neg x) \lor \alpha_{j} )$ containing respectively $x$ and $\neg x$, a new clause $r = \alpha_{i} \lor \alpha_{j}$ can be obtained by removing all occurrences of the literal $x$ and $\neg x$ in $c_{i}$ and $c_{j}$. This operation, denoted as $\eta[x, c_{i} , c_{j} ]$, is called resolution and the resulting clause $r$ is called resolvent. <d-cite key="Konlac2014"></d-cite></p> <p><strong>Subsumption</strong></p> <p>A clause $c_{i}$ subsumes another clause $c_{j}$ if and only if $c_{i} \subseteq c_{j}$. In this case, $c_{i} \models c_{j}$. <d-cite key="Konlac2014"></d-cite></p> <p><strong>Auto-subsumption</strong></p> <p>A clause $c_{i}$ auto-subsumes another clause $c_{j}$ if and only if the resolvent of $c_{i}$ and $c_{j}$ subsumes $c_{j}$. <d-cite key="Konlac2014"></d-cite></p> <p><strong>Fusion</strong></p> <p>Let $c_{i} = {x_{1} ,\ x_{2} ,\ …,\ x_{n} ,\ l,\ y_{1} ,\ y_{2} ,\ …,\ y_{m} ,\ l,\ z_{1} ,\ z_{2} ,\ …,\ z_{k} ,\ l }$ be a clause in a formula $F$, $c_{i}$ can be replaced by $c_{i} = {x_{1} ,\ x_{2} ,\ …,\ x_{n} ,\ y_{1} ,\ y_{2} ,\ …,\ y_{m} ,\ z_{1} ,\ z_{2} ,\ …,\ z_{k} ,\ l }$. This operation is called fusion. <d-cite key="Konlac2014"></d-cite></p> <hr/> <h1 id="turing-machines">Turing Machines</h1> <p>The Turing machine is an abstract concept formalizing that of an algorithm. A Turing machine operates as follows: a head being at each instant in a particular state $z_{i}$ among $Z = {z_{0},\ …,\ z_{n} }$ moves on a tape divided into an infinite number of cells, all of which are empty initially except for a finite number of them that can contain a symbol from the set $\Gamma = {s_{0},\ …,\ s_{n}}$ according to a function \(\delta : ((Z-Z_{h})\times \Gamma) \mapsto (Z\times \Gamma \times \{D,\ L,\ S\})\) $Z_{h}$ being the set of halting states of the Turing machine.</p> <hr/> <h1 id="complexity-theory">Complexity Theory</h1> <p>The temporal complexity $T$ of a problem is a measure associated with the Turing machine that can optimally solve it. It is a function of the length $n$ of the input to the Turing machine. For deterministic Turing machines, it represents the number of operations the Turing machine performs for the input of length $n$ that results in the longest calculation. It is the measure of the evolution of the number of operations to be performed depending on the length of the input:</p> \[T_{\mathcal{M}}(n)= \underset{x\in{\Sigma}^*,\ \mid x \mid =n}{max} \{k\ \mid k\ is\ the\ length\ of\ the\ computation\ for\ input\ x\}\] <p>The same goes for non-deterministic Turing machines, except here, we consider that we follow the shortest path leading to an acceptance in the resolution tree of the Turing machine:</p> \[T_{\mathcal{M}}(n)= \underset{x \in{\Sigma}^*,\ \mid x \mid =n}{max} \{min \{k\ \mid \ k\ is\ the\ length\ of\ an\ accepting\ computation\ for\ input\ x\}\}\] <p>We say that a problem is polynomial or that a language $L$ is recognized in polynomial time<d-footnote>Any decision problem can be reduced to the problem of recognizing a word in a (formal) language (this problem is undecidable in the general case for recursively enumerable languages (see the Chomsky hierarchy)), and the Turing machine that solves this problem is the Turing machine that generates this language.</d-footnote> if and only if there exists a deterministic Turing machine $\mathcal{M}$ generating this language and there exists a polynomial function $P$ such that for every $x$ in $L$, $T_{\mathcal{M}}(|x|) &lt; P(|x|)$. We thus define the set of polynomial problems $\mathsf{P}$:</p> \[\mathsf{P}= \{L \mid L\subset{\Sigma}^*,\ \exists \mathcal{M},\ L=L(\mathcal{M}),\ \exists P,\ \forall x\in L,\ T_{\mathcal{M}}(\mid x \mid) &lt; P(\mid x \mid)\}\] \[\mathcal{M}\ being\ a\ deterministic\ Turing\ machine.\] \[L\ being\ the\ language\ generated\ by\ \mathcal{M}\ (we\ write\ L=L(\mathcal{M}))\] <p>Similarly, we define the set of non-deterministic polynomial problems $\mathsf{NP}$, being the set of problems that can be solved in polynomial time by a non-deterministic Turing machine:</p> \[\mathsf{NP}= \{L \mid L\subset{\Sigma}^*,\ \exists \mathcal{M},\ L=L(\mathcal{M}),\ \exists P,\ \forall x\in L,\ T_{\mathcal{M}}(\mid x \mid) &lt; P(\mid x \mid)\}\] \[\mathcal{M}\ being\ a\ non-deterministic\ Turing\ machine.\] \[L\ being\ the\ language\ generated\ by\ \mathcal{M}\ (we\ write\ L=L(\mathcal{M}))\] <p>An interesting property of $\mathsf{NP}$ problems is that the problem of verifying the solution to an $\mathsf{NP}$ problem is itself polynomial. The algorithm that verifies a solution only consists of visiting the execution branch of the non-deterministic Turing Machine described by the solution.</p> <p>We also define the sets co-$\mathsf{P}$, the set of complements<d-footnote>A problem complementary to another problem being defined by the opposite question of that defining the latter.</d-footnote> of problems in $\mathsf{P}$, and co-$\mathsf{NP}$, also formed by the set of complements of problems in $\mathsf{NP}$. By definition, for problems in co-$\mathsf{NP}$, it is the verification of what is not a solution to the problem that can be done in polynomial time. We have $\mathsf{P}$=co-$\mathsf{P}$, we can prove it only by interchanging the two states $z_y$ and $z_n$ in the Turing machine that solves the corresponding $\mathsf{P}$ problem for each problem in co-$\mathsf{P}$.</p> <p>As for co-$\mathsf{NP}$ we know that there is an asymmetry between instances whose answer is “yes” and those whose answer is “no” for the same problem. Take, for example, the satisfiability problem; there is a polynomial-length calculation that can determine that the starting assumption is true (“yes”) (and it is the shortest path to a halt state), whereas for instances where the answer is “no”, we must go through all the paths of the Turing machine’s execution tree, and thus the complexity is exponential even in the best case.</p> <p>We don’t known whether $\mathsf{NP}$=co-$\mathsf{NP}$. The problem $\mathsf{P}$=$\mathsf{NP}$ is also still open. In fact, the conjecture $\mathsf{NP}$ $\neq$ co-$\mathsf{NP}$ is even stronger than the conjecture $\mathsf{NP}$$\neq $ $\mathsf{P}$, because if $\mathsf{P}$=$\mathsf{NP}$, then $\mathsf{P}$=co-$\mathsf{P}$=co-$\mathsf{NP}$=$\mathsf{NP}$ (indeed, if $\mathsf{P}$=$\mathsf{NP}$, then $\mathsf{P}$=co-$\mathsf{P}$=co-$\mathsf{NP}$=$\mathsf{NP}$)<d-cite key="alliot2002intelligence"></d-cite>.</p> <hr/> <h1 id="polynomial-reduction">Polynomial Reduction</h1> <p>We say that a language $L_{1} \subset \Sigma_{1}^{*} $ is polynomially reducible to another language $L_{2} \subset \Sigma_{2}^{*}$ if and only if there exists a function $f : \Sigma_{1}^{*}\to \Sigma_{2}^{*}$ such that $f$ can be computed in polynomial time by a deterministic Turing machine and for every $x\in \Sigma_{1}^{*}$, $x\in L_{1}$ if and only if $f(x)\in L_{2}$. We then denote $L_1 \propto_p L_2$. $\propto_p$ is a preorder (transitive and reflexive relation). A language $L$ is said to be $\mathsf{NP}$-complete if and only if $L$ is in $\mathsf{NP}$ and $\forall L'\in \mathsf{NP}$, $L' \propto_p L$. From this definition, we know that if we prove for only one $\mathsf{NP}$-complete language $L_0$ that $L_0\in \mathsf{N}$, then we’d have proven $\mathsf{P}=\mathsf{NP}$.</p> <p>To illustrate a polynomial reduction from an $\mathsf{NP}$ problem to SAT, we choose to briefly describe the encoding of a CSP into SAT according to <d-cite key="DBLP:conf/cp/Walsh00"></d-cite>.</p> <p>A Constraint Satisfaction Problem (CSP) consists of a set of variables $X_{i}$ each taking its value in a domain $D_{i}$ as well as a set of constraints between the values that each variable can take.</p> <p>The direct encoding associates a clause $C_{i}$ with each variable $X_{i}$ of the CSP and a propositional variable $x_{i,\ j}$ with each value $j$ in the domain $D_{i}$ of $X_{i}$. The clauses $C_{i}$ ensure that each variable $X_{i}$ has at least one assigned value. We represent the constraints between the variable values with binary clauses so, for example, if $X_{1} = 2$ and $X_{3} = 1$ is not allowed, then we add the clause $( \neg x_{1,\ 2} \lor \neg x_{3,\ 1} )$.</p>]]></content><author><name>Nazim Kerkech</name></author><summary type="html"><![CDATA[An introduction to theoretical computer science and the SAT problem]]></summary></entry></feed>